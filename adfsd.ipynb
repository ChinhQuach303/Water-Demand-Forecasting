{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e341ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HYBRID ANCHOR-RESIDUAL FRAMEWORK WITH ENHANCED EVALUATION\n",
    "Architecture: Ridge Anchor + Tree Residual + Vectorized Risk-Based Safety\n",
    "Status: UPDATED WITH DETAILED EVALUATION METRICS\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from optuna.samplers import TPESampler \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# ============= CONFIGURATION =============\n",
    "FILE_PATH = 'CaRDS.csv'\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.15\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ============= SEEDING FUNCTION =============\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Fix random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print(f\"üîí Global Seed set to: {seed}\")\n",
    "\n",
    "# ============= HYBRID ANCHOR-RESIDUAL MODEL =============\n",
    "class HybridAnchorResidualModel:\n",
    "    \"\"\"Ridge Anchor + Tree-based Residual Learning\"\"\"\n",
    "    \n",
    "    def __init__(self, ridge_alpha=1.0, residual_model='xgboost', residual_params=None, seed=42):\n",
    "        self.ridge_alpha = ridge_alpha\n",
    "        self.residual_model_name = residual_model\n",
    "        self.residual_params = residual_params or {}\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.ridge = Ridge(alpha=ridge_alpha, random_state=seed)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.residual_model = None\n",
    "        \n",
    "    def _create_residual_model(self):\n",
    "        \"\"\"Create residual model based on type with Fixed Seed\"\"\"\n",
    "        if self.residual_model_name == 'xgboost':\n",
    "            import xgboost as xgb\n",
    "            default_params = {\n",
    "                'objective': 'reg:quantileerror',\n",
    "                'quantile_alpha': 0.5,\n",
    "                'n_estimators': 3000,\n",
    "                'learning_rate': 0.02,\n",
    "                'max_depth': 6,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'reg_alpha': 0.1,\n",
    "                'reg_lambda': 1.0,\n",
    "                'random_state': self.seed,\n",
    "                'n_jobs': -1,\n",
    "                'tree_method': 'hist',\n",
    "                'early_stopping_rounds': 200\n",
    "            }\n",
    "            default_params.update(self.residual_params)\n",
    "            return xgb.XGBRegressor(**default_params)\n",
    "            \n",
    "        elif self.residual_model_name == 'lightgbm':\n",
    "            import lightgbm as lgb\n",
    "            default_params = {\n",
    "                'objective': 'quantile',\n",
    "                'alpha': 0.5,\n",
    "                'n_estimators': 3000,\n",
    "                'learning_rate': 0.02,\n",
    "                'num_leaves': 31,\n",
    "                'max_depth': 6,\n",
    "                'subsample': 0.8,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'reg_alpha': 0.1,\n",
    "                'reg_lambda': 1.0,\n",
    "                'random_state': self.seed,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            default_params.update(self.residual_params)\n",
    "            return lgb.LGBMRegressor(**default_params)\n",
    "            \n",
    "        elif self.residual_model_name == 'catboost':\n",
    "            from catboost import CatBoostRegressor\n",
    "            default_params = {\n",
    "                'loss_function': 'Quantile:alpha=0.5',\n",
    "                'iterations': 3000,\n",
    "                'learning_rate': 0.02,\n",
    "                'depth': 6,\n",
    "                'l2_leaf_reg': 1.0,\n",
    "                'early_stopping_rounds': 200,\n",
    "                'random_state': self.seed,\n",
    "                'verbose': False\n",
    "            }\n",
    "            default_params.update(self.residual_params)\n",
    "            return CatBoostRegressor(**default_params)\n",
    "            \n",
    "        elif self.residual_model_name == 'random_forest':\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            default_params = {\n",
    "                'n_estimators': 300,\n",
    "                'max_depth': 12,\n",
    "                'min_samples_split': 5,\n",
    "                'random_state': self.seed,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            default_params.update(self.residual_params)\n",
    "            return RandomForestRegressor(**default_params)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown residual model: {self.residual_model_name}\")\n",
    "    \n",
    "    def fit(self, X, y, sample_weight=None, eval_set=None, verbose=False):\n",
    "        \"\"\"Two-stage training\"\"\"\n",
    "        print(f\"   ‚öì Fitting Anchor Ridge Layer...\")\n",
    "        \n",
    "        # Stage 1: Ridge\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.ridge.fit(X_scaled, y, sample_weight=sample_weight)\n",
    "        \n",
    "        y_pred_ridge = self.ridge.predict(X_scaled)\n",
    "        residuals = y - y_pred_ridge\n",
    "        \n",
    "        ridge_mae = mean_absolute_error(y, y_pred_ridge)\n",
    "        print(f\"      Ridge MAE: {ridge_mae:,.0f}\")\n",
    "        \n",
    "        # Stage 2: Tree on residuals\n",
    "        print(f\"   üöÄ Fitting {self.residual_model_name.upper()} on Residuals...\")\n",
    "        self.residual_model = self._create_residual_model()\n",
    "        \n",
    "        # Prepare eval_set for residuals\n",
    "        eval_set_residual = None\n",
    "        if eval_set:\n",
    "            eval_set_residual = []\n",
    "            for X_val, y_val in eval_set:\n",
    "                X_val_scaled = self.scaler.transform(X_val)\n",
    "                y_val_ridge = self.ridge.predict(X_val_scaled)\n",
    "                residual_val = y_val - y_val_ridge\n",
    "                eval_set_residual.append((X_val, residual_val))\n",
    "        \n",
    "        # Fit based on model type\n",
    "        if self.residual_model_name == 'xgboost':\n",
    "            self.residual_model.fit(\n",
    "                X, residuals,\n",
    "                sample_weight=sample_weight,\n",
    "                eval_set=eval_set_residual,\n",
    "                verbose=verbose\n",
    "            )\n",
    "        elif self.residual_model_name == 'lightgbm':\n",
    "            import lightgbm as lgb\n",
    "            self.residual_model.fit(\n",
    "                X, residuals,\n",
    "                sample_weight=sample_weight,\n",
    "                eval_set=eval_set_residual,\n",
    "                callbacks=[lgb.early_stopping(200, verbose=False)]\n",
    "            )\n",
    "        elif self.residual_model_name == 'catboost':\n",
    "            self.residual_model.fit(\n",
    "                X, residuals,\n",
    "                sample_weight=sample_weight,\n",
    "                eval_set=eval_set_residual[0] if eval_set_residual else None\n",
    "            )\n",
    "        else:  # Random Forest\n",
    "            self.residual_model.fit(X, residuals, sample_weight=sample_weight)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Combine predictions\"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        pred_ridge = self.ridge.predict(X_scaled)\n",
    "        pred_residual = self.residual_model.predict(X)\n",
    "        return pred_ridge + pred_residual\n",
    "\n",
    "# ============= SAFETY LAYER V4.0 =============\n",
    "class WaterDemandSafetyLayer:\n",
    "    \"\"\"Vectorized Risk-Based Safety Layer v·ªõi c∆° ch·∫ø FAIL-SAFE.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.risk_profile = None\n",
    "        self.global_median_lag1 = 0\n",
    "    \n",
    "    def fit(self, df_val, y_true, y_pred_raw):\n",
    "        \"\"\"Learn risk profile (vectorized)\"\"\"\n",
    "        self.global_median_lag1 = df_val['lag_1'].median()\n",
    "        \n",
    "        analysis = df_val[['PWSID_enc', 'Month']].copy()\n",
    "        y_true_arr = y_true.values if hasattr(y_true, 'values') else y_true\n",
    "        analysis['Shortage'] = y_true_arr - y_pred_raw\n",
    "        \n",
    "        # 1. Error Std per station\n",
    "        grp_std = analysis.groupby('PWSID_enc')['Shortage'].std().rename('Error_Std')\n",
    "        \n",
    "        # 2. Max Historical Shortage\n",
    "        shortage_only = analysis[analysis['Shortage'] > 0]\n",
    "        grp_max = shortage_only.groupby('PWSID_enc')['Shortage'].max().rename('Max_Shortage')\n",
    "        \n",
    "        # 3. Max Summer Shortage\n",
    "        summer_months = self.config['summer_months']\n",
    "        summer_shortage = shortage_only[shortage_only['Month'].isin(summer_months)]\n",
    "        grp_max_summer = summer_shortage.groupby('PWSID_enc')['Shortage'].max().rename('Max_Summer_Shortage')\n",
    "        \n",
    "        self.risk_profile = pd.concat([grp_std, grp_max, grp_max_summer], axis=1).fillna(0)\n",
    "        self.risk_profile.index.name = 'PWSID_enc'\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, raw_pred, df_context, explain=False):\n",
    "        \"\"\"Apply vectorized safety adjustments with FAIL-SAFE CHECK\"\"\"\n",
    "        \n",
    "        # --- 0. FAIL-SAFE DETECTION ---\n",
    "        current_lag1 = df_context['lag_1'].values\n",
    "        mask_sensor_failure = np.isclose(current_lag1, self.global_median_lag1, atol=1e-3)\n",
    "        \n",
    "        # --- A. Smart Floor ---\n",
    "        cfg_f = self.config['floor']\n",
    "        lag_12 = df_context['lag_12'].values\n",
    "        lag_1 = df_context['lag_1'].values\n",
    "        months = df_context['Month'].values\n",
    "        \n",
    "        floor_yoy = lag_12 * cfg_f['yoy_growth_min']\n",
    "        \n",
    "        mom_factor = np.full_like(raw_pred, cfg_f['mom_drop_max'])\n",
    "        mask_summer = np.isin(months, self.config['summer_months'])\n",
    "        mom_factor[mask_summer] = cfg_f['mom_drop_summer']\n",
    "        mask_fall = np.isin(months, [9, 10, 11])\n",
    "        mom_factor[mask_fall] = cfg_f['mom_drop_fall']\n",
    "        \n",
    "        floor_mom = lag_1 * mom_factor\n",
    "        mask_nan = np.isnan(floor_mom)\n",
    "        floor_mom[mask_nan] = floor_yoy[mask_nan]\n",
    "        \n",
    "        floored_pred = np.maximum(raw_pred, floor_yoy)\n",
    "        floored_pred = np.maximum(floored_pred, floor_mom)\n",
    "        \n",
    "        # --- B. Adaptive Risk Buffer ---\n",
    "        if not self.config['buffer']['enabled']:\n",
    "            final_pred = floored_pred\n",
    "            buffer_vals = np.zeros_like(floored_pred)\n",
    "        else:\n",
    "            cfg_b = self.config['buffer']\n",
    "            pwsids = df_context['PWSID_enc']\n",
    "            risk_vec = self.risk_profile.reindex(pwsids).fillna(0)\n",
    "            \n",
    "            buf_base = risk_vec['Error_Std'].values * cfg_b['base_sigma']\n",
    "            buf_hist = risk_vec['Max_Shortage'].values * cfg_b['hist_coverage']\n",
    "            \n",
    "            buf_summer = np.zeros_like(buf_base)\n",
    "            buf_summer[mask_summer] = risk_vec['Max_Summer_Shortage'].values[mask_summer] * cfg_b['summer_coverage']\n",
    "            \n",
    "            raw_buffer = np.maximum(buf_base, buf_hist)\n",
    "            raw_buffer = np.maximum(raw_buffer, buf_summer)\n",
    "            \n",
    "            # === C. FAIL-SAFE INJECTION ===\n",
    "            fail_safe_add = np.zeros_like(raw_buffer)\n",
    "            fail_safe_add[mask_sensor_failure] = risk_vec['Error_Std'].values[mask_sensor_failure] * 1.5 \n",
    "            \n",
    "            total_buffer = raw_buffer + fail_safe_add\n",
    "            \n",
    "            # Cap buffer\n",
    "            cap_val = floored_pred * cfg_b['max_cap_pct']\n",
    "            cap_val[mask_sensor_failure] = cap_val[mask_sensor_failure] * 1.5\n",
    "            \n",
    "            final_buffer = np.minimum(total_buffer, cap_val)\n",
    "            \n",
    "            final_pred = floored_pred + final_buffer\n",
    "            buffer_vals = final_buffer\n",
    "        \n",
    "        if explain:\n",
    "            expl_df = pd.DataFrame({\n",
    "                'Raw': raw_pred,\n",
    "                'Floored': floored_pred,\n",
    "                'Buffer': buffer_vals,\n",
    "                'Sensor_Fail': mask_sensor_failure, \n",
    "                'Final': final_pred\n",
    "            }, index=df_context.index)\n",
    "            return final_pred, expl_df\n",
    "        \n",
    "        return final_pred\n",
    "\n",
    "# ============= DATA PROCESSING =============\n",
    "def clean_physics_based(series):\n",
    "    \"\"\"Gi·ªØ nguy√™n logic l√†m s·∫°ch d·ªØ li·ªáu ban ƒë·∫ßu\"\"\"\n",
    "    median_val = series.median()\n",
    "    if pd.isna(median_val) or median_val <= 0:\n",
    "        return series.fillna(0)\n",
    "    phys_min = median_val * 0.05\n",
    "    phys_max = median_val * 10.0\n",
    "    mask_invalid = (series < phys_min) | (series > phys_max)\n",
    "    if mask_invalid.any():\n",
    "        series_clean = series.copy()\n",
    "        series_clean[mask_invalid] = np.nan\n",
    "        return series_clean.interpolate(method='linear', limit_direction='both')\n",
    "    return series\n",
    "\n",
    "def load_and_process_data(file_path):\n",
    "    print(f\"\\nüìÇ Loading data from {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå Error: File not found.\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    df['Variable'] = df['Variable'].astype(str).str.strip().str.lower()\n",
    "    date_cols = [c for c in df.columns if c not in ['PWSID', 'Variable']]\n",
    "    df_melt = df.melt(id_vars=['PWSID', 'Variable'], value_vars=date_cols, \n",
    "                      var_name='Date', value_name='Value')\n",
    "    df_pivot = df_melt.pivot_table(index=['PWSID', 'Date'], columns='Variable', values='Value').reset_index()\n",
    "    \n",
    "    rename_map = {'demand': 'Demand', 'temperature': 'Temperature', 'precipitation': 'Precipitation', 'pdsi': 'PDSI'}\n",
    "    df_pivot.rename(columns=rename_map, inplace=True)\n",
    "    df_pivot['Date'] = pd.to_datetime(df_pivot['Date'])\n",
    "    df_final = df_pivot.sort_values(['PWSID', 'Date']).reset_index(drop=True)\n",
    "    \n",
    "    for col in ['Temperature', 'Precipitation', 'PDSI']:\n",
    "        if col in df_final.columns:\n",
    "            val = 0 if col == 'Precipitation' else df_final[col].median()\n",
    "            df_final[col] = df_final[col].fillna(val)\n",
    "    if 'Demand' in df_final.columns:\n",
    "        df_final['Demand'] = df_final.groupby('PWSID')['Demand'].transform(clean_physics_based)\n",
    "        df_final['Demand'] = df_final['Demand'].fillna(0)\n",
    "    return df_final\n",
    "\n",
    "def create_features(df):\n",
    "    print(\"\\nüî® Creating features...\")\n",
    "    df = df.copy()\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Is_Summer_Peak'] = ((df['Month'] >= 6) & (df['Month'] <= 8)).astype(int)\n",
    "    \n",
    "    if 'Temperature' in df.columns:\n",
    "        df['Temp_mean_3m'] = df.groupby('PWSID')['Temperature'].transform(lambda x: x.rolling(3, min_periods=1).mean())\n",
    "        df['CDD'] = np.maximum(df['Temperature'] - 18, 0)\n",
    "    \n",
    "    df['lag_1'] = df.groupby('PWSID')['Demand'].shift(1)\n",
    "    df['lag_12'] = df.groupby('PWSID')['Demand'].shift(12)\n",
    "    df['rolling_mean_12'] = df.groupby('PWSID')['Demand'].transform(lambda x: x.rolling(12, min_periods=1).mean())\n",
    "    df['diff_12'] = df.groupby('PWSID')['Demand'].diff(12)\n",
    "    \n",
    "    if 'Temperature' in df.columns:\n",
    "        df['Temp_lag_1'] = df.groupby('PWSID')['Temperature'].shift(1)\n",
    "        df['Summer_Heat_Interaction'] = df['Is_Summer_Peak'] * df['CDD']\n",
    "    if 'Precipitation' in df.columns:\n",
    "        df['Precip_lag_1'] = df.groupby('PWSID')['Precipitation'].shift(1)\n",
    "    return df.fillna(method='bfill').fillna(0)\n",
    "\n",
    "# ============= SAFETY OPTIMIZATION =============\n",
    "def optimize_safety_optuna(val_df, y_val, raw_pred_val, n_trials=100, seed=42):\n",
    "    \"\"\"Optuna optimization with FIXED SEED\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üöÄ STARTING OPTUNA OPTIMIZATION ({n_trials} trials) - STRICT MODE | SEED={seed}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    mean_demand = np.mean(y_val)\n",
    "    DYNAMIC_S_MAX = mean_demand * 0.005\n",
    "    target_under_rate = 0.02\n",
    "    \n",
    "    print(f\" ‚ÑπÔ∏è Auto-tuning S_MAX constraint to: {DYNAMIC_S_MAX:,.0f}\")\n",
    "    \n",
    "    val_df_sim = val_df.copy()\n",
    "    val_df_sim['lag_1'] = np.nan\n",
    "    \n",
    "    def objective(trial):\n",
    "        cfg = {\n",
    "            'summer_months': [6, 7, 8],\n",
    "            'floor': {\n",
    "                'enabled': True,\n",
    "                'yoy_growth_min': trial.suggest_float('yoy_min', 1.0, 1.25),\n",
    "                'mom_drop_max': trial.suggest_float('mom_max', 0.85, 0.99),\n",
    "                'mom_drop_summer': trial.suggest_float('mom_summer', 0.95, 1.05),\n",
    "                'mom_drop_fall': trial.suggest_float('mom_fall', 0.75, 0.95)\n",
    "            },\n",
    "            'buffer': {\n",
    "                'enabled': True,\n",
    "                'base_sigma': trial.suggest_float('base_sigma', 1.0, 3.0),\n",
    "                'hist_coverage': trial.suggest_float('hist_cov', 0.8, 1.5),\n",
    "                'summer_coverage': trial.suggest_float('summer_cov', 1.0, 2.0),\n",
    "                'max_cap_pct': trial.suggest_float('max_cap', 0.15, 0.5)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        layer = WaterDemandSafetyLayer(cfg)\n",
    "        layer.fit(val_df_sim, y_val, raw_pred_val)\n",
    "        preds = layer.predict(raw_pred_val, val_df_sim)\n",
    "        \n",
    "        diff = preds - y_val\n",
    "        surplus_score = np.mean(np.maximum(diff, 0)) / mean_demand\n",
    "        u_rate = np.mean(diff < 0)\n",
    "        s_vol = np.mean(np.maximum(-diff, 0))\n",
    "        \n",
    "        penalty = 0\n",
    "        if u_rate > target_under_rate:\n",
    "            penalty += (u_rate - target_under_rate) * 5000\n",
    "        if s_vol > DYNAMIC_S_MAX:\n",
    "            penalty += (s_vol - DYNAMIC_S_MAX) / mean_demand * 200\n",
    "        \n",
    "        return surplus_score + penalty\n",
    "    \n",
    "    sampler = TPESampler(seed=seed)\n",
    "    study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "    best = study.best_params\n",
    "    final_config = {\n",
    "        'summer_months': [6, 7, 8],\n",
    "        'floor': {\n",
    "            'enabled': True,\n",
    "            'yoy_growth_min': best['yoy_min'],\n",
    "            'mom_drop_max': best['mom_max'],\n",
    "            'mom_drop_summer': best['mom_summer'],\n",
    "            'mom_drop_fall': best['mom_fall']\n",
    "        },\n",
    "        'buffer': {\n",
    "            'enabled': True,\n",
    "            'base_sigma': best['base_sigma'],\n",
    "            'hist_coverage': best['hist_cov'],\n",
    "            'summer_coverage': best['summer_cov'],\n",
    "            'max_cap_pct': best['max_cap']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Optimization complete! Best score: {study.best_value:.6f}\")\n",
    "    return final_config\n",
    "\n",
    "# ============= ƒê√ÅNH GI√Å CHI TI·∫æT =============\n",
    "def detailed_evaluation(y_true, y_pred, name):\n",
    "    \"\"\"\n",
    "    T√≠nh to√°n metrics trung b√¨nh tr√™n m·ªói m·∫´u:\n",
    "    MAE, R2, % Shortage, Avg Shortage Vol, Avg Surplus Vol\n",
    "    \"\"\"\n",
    "    y_true_arr = np.array(y_true).ravel()\n",
    "    y_pred_arr = np.array(y_pred).ravel()\n",
    "    diff = y_true_arr - y_pred_arr\n",
    "    total_samples = len(y_true_arr)\n",
    "    \n",
    "    # 1. Accuracy\n",
    "    mae = mean_absolute_error(y_true_arr, y_pred_arr)\n",
    "    r2 = r2_score(y_true_arr, y_pred_arr)\n",
    "    \n",
    "    # 2. Shortage (Th·ª±c t·∫ø > D·ª± b√°o)\n",
    "    shortage_mask = diff > 0\n",
    "    shortage_count = np.sum(shortage_mask)\n",
    "    shortage_pct = (shortage_count / total_samples) * 100\n",
    "    # L∆∞·ª£ng thi·∫øu h·ª•t trung b√¨nh tr√™n T·ªîNG s·ªë m·∫´u\n",
    "    avg_shortage_vol = np.sum(diff[shortage_mask]) / total_samples\n",
    "    \n",
    "    # 3. Surplus (D·ª± b√°o > Th·ª±c t·∫ø)\n",
    "    surplus_mask = diff < 0\n",
    "    # L∆∞·ª£ng d∆∞ th·ª´a trung b√¨nh tr√™n T·ªîNG s·ªë m·∫´u\n",
    "    avg_surplus_vol = np.sum(np.abs(diff[surplus_mask])) / total_samples\n",
    "    \n",
    "    metrics = {\n",
    "        'Dataset': name,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'Shortage_%': shortage_pct,\n",
    "        'Avg_Shortage': avg_shortage_vol,\n",
    "        'Avg_Surplus': avg_surplus_vol\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä RESULTS FOR: {name}\")\n",
    "    print(f\"--------------------------------------\")\n",
    "    print(f\"üìà MAE              : {mae:,.2f}\")\n",
    "    print(f\"üìà R2 Score         : {r2:.4f}\")\n",
    "    print(f\"üî¥ Shortage %       : {shortage_pct:.2f}%\")\n",
    "    print(f\"üî¥ Avg Shortage Vol : {avg_shortage_vol:,.2f}\")\n",
    "    print(f\"üü¢ Avg Surplus Vol  : {avg_surplus_vol:,.2f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ============= MAIN PIPELINE =============\n",
    "def main(residual_model='xgboost', ridge_alpha=2.0, n_trials=50, seed=42):\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # 1. Data Prep\n",
    "    df = load_and_process_data(FILE_PATH)\n",
    "    if df is None: return None\n",
    "    df_features = create_features(df)\n",
    "    \n",
    "    # Split\n",
    "    unique_dates = df_features['Date'].sort_values().unique()\n",
    "    n_test, n_val = int(len(unique_dates)*TEST_SIZE), int(len(unique_dates)*VAL_SIZE)\n",
    "    test_start, val_start = unique_dates[-n_test], unique_dates[-(n_test+n_val)]\n",
    "    \n",
    "    train_df = df_features[df_features['Date'] < val_start].copy()\n",
    "    val_df = df_features[(df_features['Date'] >= val_start) & (df_features['Date'] < test_start)].copy()\n",
    "    test_df = df_features[df_features['Date'] >= test_start].copy()\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    train_df['PWSID_enc'] = le.fit_transform(train_df['PWSID'])\n",
    "    val_df['PWSID_enc'] = le.transform(val_df['PWSID'])\n",
    "    test_df['PWSID_enc'] = le.transform(test_df['PWSID'])\n",
    "    \n",
    "    features = ['PWSID_enc', 'Month', 'Year', 'Is_Summer_Peak', 'lag_1', 'lag_12', 'diff_12', 'CDD']\n",
    "    X_train, y_train = train_df[features], train_df['Demand']\n",
    "    X_val, y_val = val_df[features], val_df['Demand']\n",
    "    X_test, y_test = test_df[features], test_df['Demand']\n",
    "    \n",
    "    # 2. Train Hybrid Model\n",
    "    print(f\"\\nüöÄ TRAINING {residual_model.upper()} HYBRID...\")\n",
    "    model = HybridAnchorResidualModel(ridge_alpha=ridge_alpha, residual_model=residual_model, seed=seed)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "    \n",
    "    # 3. Raw Predictions & Debug\n",
    "    raw_pred_test = model.predict(X_test)\n",
    "    raw_pred_val = model.predict(X_val)\n",
    "    raw_pred_train = model.predict(X_train)\n",
    "    \n",
    "    # In Raw Metrics ƒë·ªÉ so s√°nh\n",
    "    print(\"\\n--- RAW MODEL PERFORMANCE (BEFORE SAFETY) ---\")\n",
    "    raw_metrics = detailed_evaluation(y_test, raw_pred_test, f\"RAW_{residual_model.upper()}\")\n",
    "\n",
    "    # 4. Safety Optimization\n",
    "    cfg = optimize_safety_optuna(val_df, y_val, raw_pred_val, n_trials=n_trials, seed=seed)\n",
    "    final_layer = WaterDemandSafetyLayer(cfg)\n",
    "    final_layer.fit(val_df, y_val, raw_pred_val)\n",
    "    \n",
    "    # 5. Final Predictions\n",
    "    final_pred_train = final_layer.predict(raw_pred_train, train_df)\n",
    "    final_pred_val = final_layer.predict(raw_pred_val, val_df)\n",
    "    final_pred_test = final_layer.predict(raw_pred_test, test_df)\n",
    "    \n",
    "    # 6. Final Evaluation\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üì¢ FINAL PERFORMANCE EVALUATION (WITH SAFETY)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    report_train = detailed_evaluation(y_train, final_pred_train, \"TRAIN\")\n",
    "    report_val = detailed_evaluation(y_val, final_pred_val, \"VALIDATION\")\n",
    "    report_test = detailed_evaluation(y_test, final_pred_test, \"TEST (UNSEEN)\")\n",
    "    \n",
    "    # 7. Summary Table\n",
    "    summary = pd.DataFrame([report_train, report_val, report_test])\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã SUMMARY TABLE (AVERAGE METRICS)\")\n",
    "    print(\"=\"*70)\n",
    "    # ƒê·ªãnh d·∫°ng hi·ªÉn th·ªã s·ªë th·ª±c\n",
    "    pd.options.display.float_format = '{:,.2f}'.format\n",
    "    print(summary.to_string(index=False))\n",
    "    \n",
    "    return {'model': model, 'safety': final_layer, 'summary': summary}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test l·∫ßn l∆∞·ª£t ƒë·ªÉ th·∫•y s·ª± kh√°c bi·ªát c·ªßa metrics\n",
    "    results = main(residual_model='xgboost', ridge_alpha=2.0, n_trials=50, seed=42)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
