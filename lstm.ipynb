{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650e633d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ADAPTIVE LSTM V2: CLASS-BASED SAFETY LAYER\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Architecture: Deep Learning Median -> Smart Floor -> Adaptive Risk Buffer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/__init__.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m hard_dependencies, dependency, missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/compat/__init__.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     is_numpy_dev,\n\u001b[1;32m     17\u001b[0m     np_version_under1p19,\n\u001b[1;32m     18\u001b[0m     np_version_under1p20,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     pa_version_under1p01,\n\u001b[1;32m     22\u001b[0m     pa_version_under2p0,\n\u001b[1;32m     23\u001b[0m     pa_version_under3p0,\n\u001b[1;32m     24\u001b[0m     pa_version_under4p0,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m PY39 \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/compat/numpy/__init__.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[1;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     Appender,\n\u001b[1;32m      3\u001b[0m     Substitution,\n\u001b[1;32m      4\u001b[0m     cache_readonly,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     hash_array,\n\u001b[1;32m      9\u001b[0m     hash_pandas_object,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(name):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Any,\n\u001b[1;32m      8\u001b[0m     Callable,\n\u001b[1;32m      9\u001b[0m     Mapping,\n\u001b[1;32m     10\u001b[0m     cast,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cache_readonly  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeprecate\u001b[39m(\n\u001b[1;32m     19\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     20\u001b[0m     alternative: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[[F], F]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/__init__.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ADAPTIVE LSTM V2: CLASS-BASED SAFETY LAYER\n",
    "# Architecture: Deep Learning Median -> Smart Floor -> Adaptive Risk Buffer\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "FILE_PATH = \"CaRDS.csv\"\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.15\n",
    "SEQUENCE_LENGTH = 14\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "QUANTILE = 0.5 # Median Prediction\n",
    "\n",
    "# Cáº¤U HÃŒNH Lá»šP AN TOÃ€N\n",
    "SAFETY_CONFIG = {\n",
    "    'summer_months': [6, 7, 8],\n",
    "    'floor': {\n",
    "        'enabled': True,\n",
    "        'yoy_growth_min': 1.02,\n",
    "        'mom_drop_max': 0.92,\n",
    "        'mom_drop_summer': 0.95,\n",
    "        'mom_drop_fall': 0.75\n",
    "    },\n",
    "    'buffer': {\n",
    "        'enabled': True,\n",
    "        'base_sigma': 2.0,\n",
    "        'hist_coverage': 1.05,\n",
    "        'summer_coverage': 1.15,\n",
    "        'max_cap_pct': 0.50\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ADAPTIVE LSTM PIPELINE WITH SAFETY LAYER V2\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =============================================================================\n",
    "# 0. SAFETY LAYER CLASS\n",
    "# =============================================================================\n",
    "class WaterDemandSafetyLayer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.risk_map = {} \n",
    "\n",
    "    def fit(self, df_val, y_true, y_pred_raw):\n",
    "        print(\"   âš™ï¸ Fitting Safety Layer (Learning Risk Profiles)...\")\n",
    "        analysis = df_val[['PWSID_enc', 'Month']].copy()\n",
    "        analysis['Actual'] = y_true.values if hasattr(y_true, 'values') else y_true\n",
    "        analysis['Pred'] = y_pred_raw\n",
    "        analysis['Shortage'] = analysis['Actual'] - analysis['Pred']\n",
    "        \n",
    "        risk_profile = analysis.groupby('PWSID_enc')['Shortage'].std().reset_index()\n",
    "        risk_profile.rename(columns={'Shortage': 'Error_Std'}, inplace=True)\n",
    "        \n",
    "        shortage_only = analysis[analysis['Shortage'] > 0]\n",
    "        if not shortage_only.empty:\n",
    "            max_risk = shortage_only.groupby('PWSID_enc')['Shortage'].max().reset_index()\n",
    "            max_risk.rename(columns={'Shortage': 'Max_Shortage'}, inplace=True)\n",
    "        else:\n",
    "            max_risk = pd.DataFrame(columns=['PWSID_enc', 'Max_Shortage'])\n",
    "            \n",
    "        summer_months = self.config['summer_months']\n",
    "        summer_data = analysis[analysis['Month'].isin(summer_months)]\n",
    "        summer_shortage = summer_data[summer_data['Shortage'] > 0]\n",
    "        if not summer_shortage.empty:\n",
    "            max_summer_risk = summer_shortage.groupby('PWSID_enc')['Shortage'].max().reset_index()\n",
    "            max_summer_risk.rename(columns={'Shortage': 'Max_Summer_Shortage'}, inplace=True)\n",
    "        else:\n",
    "            max_summer_risk = pd.DataFrame(columns=['PWSID_enc', 'Max_Summer_Shortage'])\n",
    "\n",
    "        profile = risk_profile.merge(max_risk, on='PWSID_enc', how='left')\n",
    "        profile = profile.merge(max_summer_risk, on='PWSID_enc', how='left')\n",
    "        profile = profile.fillna(0)\n",
    "        self.risk_map = profile.set_index('PWSID_enc').to_dict('index')\n",
    "        print(f\"      -> Learned profiles for {len(self.risk_map)} stations.\")\n",
    "\n",
    "    def _apply_guardrails(self, base_pred, df_context):\n",
    "        if not self.config['floor']['enabled']:\n",
    "            return base_pred, np.zeros_like(base_pred, dtype=bool)\n",
    "\n",
    "        cfg = self.config['floor']\n",
    "        lag_12 = df_context['lag_12'].values\n",
    "        lag_1 = df_context['lag_1'].values\n",
    "        months = df_context['Month'].values\n",
    "        \n",
    "        floor_yoy = lag_12 * cfg['yoy_growth_min']\n",
    "        floor_mom = lag_1 * cfg['mom_drop_max'] \n",
    "        \n",
    "        summer_mask = np.isin(months, self.config['summer_months'])\n",
    "        floor_mom[summer_mask] = lag_1[summer_mask] * cfg['mom_drop_summer']\n",
    "        \n",
    "        fall_mask = np.isin(months, [9, 10, 11])\n",
    "        floor_mom[fall_mask] = lag_1[fall_mask] * cfg['mom_drop_fall']\n",
    "        \n",
    "        floored_pred = np.maximum.reduce([base_pred, floor_yoy, floor_mom])\n",
    "        is_floored = floored_pred > (base_pred + 1e-3)\n",
    "        return floored_pred, is_floored\n",
    "\n",
    "    def _apply_risk_buffer(self, pred, df_context):\n",
    "        if not self.config['buffer']['enabled']:\n",
    "            return pred, np.zeros_like(pred)\n",
    "\n",
    "        cfg = self.config['buffer']\n",
    "        pwsid_encs = df_context['PWSID_enc'].values\n",
    "        months = df_context['Month'].values\n",
    "        \n",
    "        buffers = []\n",
    "        for i, pid in enumerate(pwsid_encs):\n",
    "            profile = self.risk_map.get(pid, {'Error_Std': 0, 'Max_Shortage': 0, 'Max_Summer_Shortage': 0})\n",
    "            \n",
    "            base_buf = cfg['base_sigma'] * profile['Error_Std']\n",
    "            hist_buf = profile['Max_Shortage'] * cfg['hist_coverage']\n",
    "            \n",
    "            summer_buf = 0\n",
    "            if months[i] in self.config['summer_months']:\n",
    "                summer_buf = profile['Max_Summer_Shortage'] * cfg['summer_coverage']\n",
    "            \n",
    "            final_buf = max(base_buf, hist_buf, summer_buf)\n",
    "            \n",
    "            if pred[i] > 0:\n",
    "                cap_val = pred[i] * cfg['max_cap_pct']\n",
    "                if final_buf > cap_val and final_buf > profile['Max_Shortage'] * 1.1:\n",
    "                    final_buf = cap_val\n",
    "            \n",
    "            buffers.append(final_buf)\n",
    "            \n",
    "        return pred + np.array(buffers), np.array(buffers)\n",
    "\n",
    "    def predict(self, raw_pred, df_context, explain=False):\n",
    "        floored_pred, is_floored = self._apply_guardrails(raw_pred, df_context)\n",
    "        final_pred, buffer_amount = self._apply_risk_buffer(floored_pred, df_context)\n",
    "        \n",
    "        if explain:\n",
    "            explanation_df = df_context.copy()\n",
    "            explanation_df['Raw_Model'] = raw_pred\n",
    "            explanation_df['Floored_Pred'] = floored_pred\n",
    "            explanation_df['Is_Floored'] = is_floored\n",
    "            explanation_df['Risk_Buffer'] = buffer_amount\n",
    "            explanation_df['Final_Forecast'] = final_pred\n",
    "            return final_pred, explanation_df\n",
    "        return final_pred\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATA & FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "def clean_physics_based(series):\n",
    "    median_val = series.median()\n",
    "    if pd.isna(median_val) or median_val <= 0: return series.fillna(0)\n",
    "    phys_min = median_val * 0.05\n",
    "    phys_max = median_val * 10.0\n",
    "    mask_invalid = (series < phys_min) | (series > phys_max)\n",
    "    if mask_invalid.any():\n",
    "        series_clean = series.copy()\n",
    "        series_clean[mask_invalid] = np.nan\n",
    "        return series_clean.interpolate(method='linear', limit_direction='both')\n",
    "    return series\n",
    "\n",
    "def load_and_process_data(file_path):\n",
    "    print(f\"\\nðŸ“‚ Loading data from {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âŒ Error: File not found.\")\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "    df['Variable'] = df['Variable'].astype(str).str.strip().str.lower()\n",
    "    date_cols = [c for c in df.columns if c not in ['PWSID', 'Variable']]\n",
    "    df_melt = df.melt(id_vars=['PWSID', 'Variable'], value_vars=date_cols, var_name='Date', value_name='Value')\n",
    "    df_pivot = df_melt.pivot_table(index=['PWSID', 'Date'], columns='Variable', values='Value').reset_index()\n",
    "    \n",
    "    rename_map = {'demand': 'Demand', 'temperature': 'Temperature', 'precipitation': 'Precipitation', 'pdsi': 'PDSI'}\n",
    "    df_pivot.rename(columns=rename_map, inplace=True)\n",
    "    df_pivot['Date'] = pd.to_datetime(df_pivot['Date'])\n",
    "    df_final = df_pivot.sort_values(['PWSID', 'Date']).reset_index(drop=True)\n",
    "    \n",
    "    for col in ['Temperature', 'Precipitation', 'PDSI']:\n",
    "        if col in df_final.columns:\n",
    "            val = 0 if col == 'Precipitation' else df_final[col].median()\n",
    "            df_final[col] = df_final[col].fillna(val)\n",
    "    \n",
    "    if 'Demand' in df_final.columns:\n",
    "        df_final['Demand'] = df_final.groupby('PWSID')['Demand'].transform(clean_physics_based)\n",
    "        df_final['Demand'] = df_final['Demand'].fillna(0)\n",
    "    return df_final\n",
    "\n",
    "df = load_and_process_data(FILE_PATH)\n",
    "if df is None: exit()\n",
    "\n",
    "# Feature Engineering\n",
    "print(\"\\nðŸ”¨ Creating features for LSTM...\")\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "df['Is_Summer_Peak'] = ((df['Month'] >= 6) & (df['Month'] <= 8)).astype(int)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['PWSID_enc'] = le.fit_transform(df['PWSID'])\n",
    "\n",
    "# Scaling (Only continuous vars)\n",
    "scale_cols = ['Demand', 'Temperature', 'Precipitation', 'month_sin', 'month_cos']\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df.copy()\n",
    "df_scaled[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
    "\n",
    "feature_cols = scale_cols + ['PWSID_enc']\n",
    "demand_mean = scaler.mean_[0]\n",
    "demand_std = scaler.scale_[0]\n",
    "\n",
    "# Sequence Creation\n",
    "def create_sequences(df_scaled, df_raw, seq_len, feature_cols):\n",
    "    X, y = [], []\n",
    "    meta_info = [] \n",
    "    \n",
    "    unique_ids = df_scaled['PWSID_enc'].unique()\n",
    "    for pid in unique_ids:\n",
    "        group = df_scaled[df_scaled['PWSID_enc'] == pid]\n",
    "        group_raw = df_raw[df_raw['PWSID_enc'] == pid] \n",
    "        \n",
    "        vals = group[feature_cols].values\n",
    "        target = group['Demand'].values \n",
    "        dates = group_raw['Date'].values\n",
    "        months = group_raw['Month'].values\n",
    "        \n",
    "        if len(group) <= seq_len: continue\n",
    "            \n",
    "        for i in range(len(group) - seq_len):\n",
    "            X.append(vals[i : i+seq_len])\n",
    "            y.append(target[i+seq_len])\n",
    "            meta_info.append({\n",
    "                'PWSID_enc': pid,\n",
    "                'Date': dates[i+seq_len],\n",
    "                'Month': months[i+seq_len],\n",
    "                'Actual_Raw': df_raw[(df_raw['PWSID_enc']==pid) & (df_raw['Date']==dates[i+seq_len])]['Demand'].values[0]\n",
    "            })\n",
    "            \n",
    "    return np.array(X), np.array(y), pd.DataFrame(meta_info)\n",
    "\n",
    "print(\"   Processing sequences...\")\n",
    "X_all, y_all, meta_df = create_sequences(df_scaled, df, SEQUENCE_LENGTH, feature_cols)\n",
    "\n",
    "# Split\n",
    "unique_dates = meta_df['Date'].sort_values().unique()\n",
    "n_test = int(len(unique_dates) * TEST_SIZE)\n",
    "n_val = int(len(unique_dates) * VAL_SIZE)\n",
    "test_start = unique_dates[-n_test]\n",
    "val_start = unique_dates[-(n_test + n_val)]\n",
    "\n",
    "train_mask = meta_df['Date'] < val_start\n",
    "val_mask = (meta_df['Date'] >= val_start) & (meta_df['Date'] < test_start)\n",
    "test_mask = meta_df['Date'] >= test_start\n",
    "\n",
    "X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
    "X_val, y_val = X_all[val_mask], y_all[val_mask]\n",
    "X_test, y_test = X_all[test_mask], y_all[test_mask]\n",
    "\n",
    "meta_train = meta_df[train_mask].reset_index(drop=True)\n",
    "meta_val = meta_df[val_mask].reset_index(drop=True)\n",
    "meta_test = meta_df[test_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"   Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. TRAINING LSTM\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: TRAINING LSTM (Quantile Loss Alpha=0.5)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def quantile_loss(q, y_true, y_pred):\n",
    "    e = y_true - y_pred\n",
    "    return tf.reduce_mean(tf.maximum(q*e, (q-1)*e))\n",
    "\n",
    "def loss_func(y_true, y_pred):\n",
    "    return quantile_loss(QUANTILE, y_true, y_pred)\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    LSTM(64, return_sequences=True, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=loss_func)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)\n",
    "\n",
    "print(\"   â³ Training LSTM Network...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict & Inverse\n",
    "pred_scaled_train = model.predict(X_train)\n",
    "pred_scaled_val = model.predict(X_val)\n",
    "pred_scaled_test = model.predict(X_test)\n",
    "\n",
    "def inverse(y_scaled):\n",
    "    return (y_scaled.flatten() * demand_std) + demand_mean\n",
    "\n",
    "raw_pred_train = inverse(pred_scaled_train)\n",
    "raw_pred_val = inverse(pred_scaled_val)\n",
    "raw_pred_test = inverse(pred_scaled_test)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. APPLYING SAFETY LAYER\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2 & 3: APPLYING WATER DEMAND SAFETY LAYER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# IMPORTANT: Re-create lags for the meta DataFrame because sequence splitting broke continuity\n",
    "def recreate_lags(df):\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['PWSID_enc', 'Date'])\n",
    "    df['lag_1'] = df.groupby('PWSID_enc')['Actual_Raw'].shift(1)\n",
    "    df['lag_12'] = df.groupby('PWSID_enc')['Actual_Raw'].shift(12)\n",
    "    df = df.fillna(method='bfill')\n",
    "    return df\n",
    "\n",
    "meta_train = recreate_lags(meta_train)\n",
    "meta_val = recreate_lags(meta_val)\n",
    "meta_test = recreate_lags(meta_test)\n",
    "\n",
    "# Initialize Safety Layer\n",
    "safety_layer = WaterDemandSafetyLayer(config=SAFETY_CONFIG)\n",
    "\n",
    "# 1. Fit (Using Actual_Raw from meta_df)\n",
    "safety_layer.fit(meta_val, meta_val['Actual_Raw'], raw_pred_val)\n",
    "\n",
    "# 2. Predict & Explain\n",
    "print(\"   ðŸš€ Generating forecasts with safety margins...\")\n",
    "final_val_pred, val_explain = safety_layer.predict(raw_pred_val, meta_val, explain=True)\n",
    "final_test_pred, test_explain = safety_layer.predict(raw_pred_test, meta_test, explain=True)\n",
    "\n",
    "print(\"\\nðŸ” AUDIT LOG SAMPLE (Test Set):\")\n",
    "print(test_explain.head(3)[['Raw_Model', 'Floored_Pred', 'Risk_Buffer', 'Final_Forecast']])\n",
    "\n",
    "# =============================================================================\n",
    "# 5. EVALUATION & VISUALIZATION (LSTM SPECIALIZED)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL EVALUATION & VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Mean Map\n",
    "profile_df = pd.concat([meta_train, meta_val])\n",
    "mean_map = profile_df.groupby('PWSID_enc')['Actual_Raw'].mean().to_dict()\n",
    "\n",
    "# 2. Evaluation\n",
    "def evaluate_model(y_true, y_pred, df_meta, dataset_name):\n",
    "    if hasattr(y_true, 'values'): y_true = y_true.values\n",
    "        \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    threshold = pd.Series(y_true).quantile(0.1)\n",
    "    mask = y_true > threshold\n",
    "    if mask.sum() > 0:\n",
    "        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / (y_true[mask] + 1e-10))) * 100\n",
    "    else:\n",
    "        mape = 0.0\n",
    "    \n",
    "    under_pred_count = np.sum(y_pred < y_true)\n",
    "    under_rate = (under_pred_count / len(y_true)) * 100\n",
    "    over_rate = 100 - under_rate\n",
    "    avg_shortage = np.mean(np.maximum(y_true - y_pred, 0))\n",
    "    avg_surplus = np.mean(np.maximum(y_pred - y_true, 0))\n",
    "    \n",
    "    months = df_meta['Month'].values\n",
    "    summer_mask = (months >= 6) & (months <= 8)\n",
    "    if np.sum(summer_mask) > 0:\n",
    "        summer_under = np.sum((y_pred[summer_mask] < y_true[summer_mask])) / np.sum(summer_mask) * 100\n",
    "    else:\n",
    "        summer_under = 0.0\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {dataset_name}:\")\n",
    "    print(f\"   MAE: {mae:,.2f} | RÂ²: {r2:.4f}\")\n",
    "    print(f\"   MAPE (filtered): {mape:.2f}%\")\n",
    "    print(f\"   âš ï¸  Under-pred: {under_rate:.1f}% | Avg Shortage: {avg_shortage:,.2f}\")\n",
    "    print(f\"   âœ… Over-pred: {over_rate:.1f}% | Avg Surplus: {avg_surplus:,.2f}\")\n",
    "    print(f\"   ðŸŒž Summer Under-pred: {summer_under:.1f}% (CRITICAL METRIC)\")\n",
    "    return {'R2': r2}\n",
    "\n",
    "val_metrics = evaluate_model(meta_val['Actual_Raw'], final_pred_val, meta_val, \"VALIDATION SET\")\n",
    "test_metrics = evaluate_model(meta_test['Actual_Raw'], final_pred_test, meta_test, \"TEST SET\")\n",
    "\n",
    "# 3. Visualization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VISUALIZING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "val_plot = val_explain.copy()\n",
    "val_plot['Actual'] = val_plot['Actual_Raw']\n",
    "val_monthly = val_plot.groupby('Date')[['Actual', 'Final_Forecast']].sum().reset_index()\n",
    "\n",
    "test_plot = test_explain.copy()\n",
    "test_plot['Actual'] = test_plot['Actual_Raw']\n",
    "test_monthly = test_plot.groupby('Date')[['Actual', 'Final_Forecast']].sum().reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "axes[0].plot(val_monthly['Date'], val_monthly['Actual'], 'k-o', label='Actual', alpha=0.7)\n",
    "axes[0].plot(val_monthly['Date'], val_monthly['Final_Forecast'], 'g--s', label='Predicted (Adaptive LSTM)', alpha=0.8)\n",
    "axes[0].fill_between(val_monthly['Date'], val_monthly['Final_Forecast'], val_monthly['Actual'],\n",
    "                     where=(val_monthly['Final_Forecast'] < val_monthly['Actual']),\n",
    "                     color='red', alpha=0.3, label='Shortage Risk')\n",
    "axes[0].set_title(f\"Validation Set (R2={val_metrics['R2']:.3f})\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(test_monthly['Date'], test_monthly['Actual'], 'k-o', label='Actual', alpha=0.7)\n",
    "axes[1].plot(test_monthly['Date'], test_monthly['Final_Forecast'], 'b--s', label='Predicted (Adaptive LSTM)', alpha=0.8)\n",
    "axes[1].fill_between(test_monthly['Date'], test_monthly['Final_Forecast'], test_monthly['Actual'],\n",
    "                     where=(test_monthly['Final_Forecast'] < test_monthly['Actual']),\n",
    "                     color='red', alpha=0.3, label='Shortage Risk')\n",
    "axes[1].set_title(f\"Test Set (R2={test_metrics['R2']:.3f})\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Insight Plot\n",
    "print(\"\\nGenerating Margin Efficiency Plot...\")\n",
    "test_explain['Demand'] = test_explain['Actual_Raw']\n",
    "test_explain['Margin_Pct'] = ((test_explain['Final_Forecast'] - test_explain['Demand']) / (test_explain['Demand']+1)) * 100\n",
    "test_explain['Mean_Volume'] = test_explain['PWSID_enc'].map(mean_map)\n",
    "test_explain['Is_Summer_Peak'] = ((test_explain['Month'] >= 6) & (test_explain['Month'] <= 8)).astype(int)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_data = test_explain[(test_explain['Margin_Pct'] > -50) & (test_explain['Margin_Pct'] < 150)]\n",
    "\n",
    "sns.scatterplot(data=plot_data, x='Mean_Volume', y='Margin_Pct', hue='Is_Summer_Peak', alpha=0.4, palette='viridis')\n",
    "plt.xscale('log')\n",
    "plt.axhline(0, color='red', linestyle='--', label='Zero Shortage Line')\n",
    "plt.title('Adaptive LSTM: Safety Margins by Volume')\n",
    "plt.xlabel('Mean Demand (Log Scale)')\n",
    "plt.ylabel('Safety Margin (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… PROCESS COMPLETE - ADAPTIVE LSTM\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
